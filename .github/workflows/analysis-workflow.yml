name: 'SonarCloud Historical Issue Analysis (Parallel)'

on:
  workflow_dispatch:

jobs:
  # Job 1: Splits the large issues.json file into smaller, manageable chunks.
  setup-chunks:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.create-matrix.outputs.matrix }}
    steps:
      - name: 'Checkout repository'
        uses: actions/checkout@v4

      - name: 'Install jq'
        run: sudo apt-get install -y jq

      - name: 'Debug: Check issues.json file'
        run: |
          if [ ! -f "issues.json" ]; then
            echo "::error::issues.json file not found!"
            exit 1
          fi
          echo "issues.json found. First 10 lines:"
          head -n 10 issues.json

      - name: 'Split JSON into chunks'
        id: create-matrix
        run: |
          mkdir -p chunks
          # Step 1: Process the JSON and save the output to a temporary file, one chunk per line.
          jq -c '. | to_entries | group_by(.key / 5 | floor) | .[] | map(.value)' issues.json > chunks.tmp

          # Step 2: Debugging - check if the temporary file has content.
          if [ ! -s chunks.tmp ]; then
            echo "::error::jq processing resulted in an empty file. Check issues.json structure."
            exit 1
          fi
          echo "Temporary chunk file created with $(wc -l < chunks.tmp) chunks."

          # Step 3: Read the temporary file and create the final chunk files.
          i=0
          while read -r chunk; do
            printf -v filename "chunks/part-%02d.json" "$i"
            echo "$chunk" > "$filename"
            echo "Created chunk file: $filename"
            i=$((i+1))
          done < chunks.tmp

          # Clean up the temporary file
          rm chunks.tmp
          
          # Check if any chunk files were created. If not, fail with an error.
          if [ ! "$(ls -A chunks)" ]; then
            echo "::error::No chunk files were created. issues.json might be empty or invalid."
            exit 1
          fi

          # Create a compact, single-line JSON array of the chunk filenames for the matrix strategy.
          matrix_json=$(ls chunks | jq -R . | jq -s -c .)
          echo "Generated matrix for next job: $matrix_json"
          echo "matrix=${matrix_json}" >> $GITHUB_OUTPUT

      - name: 'Upload chunks artifact'
        uses: actions/upload-artifact@v4
        with:
          name: issue-chunks
          path: chunks/

  # Job 2: Runs in parallel for each chunk file created above.
  analyze-chunk:
    runs-on: ubuntu-latest
    needs: setup-chunks
    strategy:
      fail-fast: false
      matrix:
        # The matrix is dynamically created from the list of chunk filenames.
        chunk_file: ${{ fromJson(needs.setup-chunks.outputs.matrix) }}

    steps:
      - name: 'Download issue chunks'
        uses: actions/download-artifact@v4
        with:
          name: issue-chunks
          path: chunks

      - name: 'Create directory for reports'
        run: mkdir -p chunk-reports

      - name: 'Create a local mirror of the repository'
        run: git clone --mirror "https://github.com/${{ github.repository }}.git" repo.mirror

      - name: 'Process each issue in a loop'
        run: |
          chunk_file="chunks/${{ matrix.chunk_file }}"
          # FIX: Use a more robust 'for' loop that iterates by index instead of 'while read'.
          # This avoids potential parsing errors with complex lines.
          issue_count=$(jq '. | length' "$chunk_file")
          for (( i=0; i<$issue_count; i++ )); do
            # Extract the i-th issue object from the chunk file
            issue=$(jq -c ".[$i]" "$chunk_file")

            # Extract details for the current issue using jq
            issue_number=$(echo "$issue" | jq -r '.number')
            base_hash=$(echo "$issue" | jq -r '.base_hash')
            new_hash=$(echo "$issue" | jq -r '.new_hash')
            
            # Create a dedicated, isolated directory for this issue's analysis
            work_dir="work/${issue_number}"
            
            echo "--- [${{ matrix.chunk_file }}] Processing Issue #${issue_number} (Index $i) ---"
            
            # Clone from the local mirror for speed and completeness.
            echo "Cloning from local mirror into ${work_dir}..."
            git clone ./repo.mirror "${work_dir}"
            cd "${work_dir}"

            # Use status files to track success/failure
            echo "failure" > ../../base_status.txt
            echo "failure" > ../../new_status.txt
            
            base_commit_date=""

            # --- START: BASE HASH ANALYSIS (Version 1.0) ---
            if git rev-parse --verify "$base_hash^{commit}" >/dev/null 2>&1; then
              echo "Analyzing Base Commit: ${base_hash}"
              git checkout -f "$base_hash"
              base_commit_date=$(git log -1 --format=%as)
              
              build_succeeded=false
              if mvn -B install -DskipTests -Drat.skip=true; then build_succeeded=true; fi
              if [ "$build_succeeded" = false ]; then
                echo "Initial build failed for base. Retrying with compile..."
                if mvn -B compile -Drat.skip=true; then build_succeeded=true; fi
              fi

              if [ "$build_succeeded" = true ]; then
                echo "Build succeeded for base. Running Sonar analysis..."
                if mvn -B sonar:sonar -Drat.skip=true -Dsonar.host.url=https://sonarcloud.io -Dsonar.projectKey=maven-issue-${issue_number} -Dsonar.projectName="Maven Issue #${issue_number}" -Dsonar.organization=${{ vars.SONAR_ORGANIZATION }} -Dsonar.projectVersion=1.0 -Dsonar.login=${{ secrets.SONAR_TOKEN }} -Dsonar.projectDate=${base_commit_date}; then
                  echo "success" > ../../base_status.txt
                fi
              else
                echo "::error::All build attempts failed for base hash ${base_hash}."
              fi
            else
              echo "::warning::Base commit ${base_hash} not found in repository mirror. Skipping."
            fi

            # --- START: NEW HASH ANALYSIS (Version 2.0) ---
            if git rev-parse --verify "$new_hash^{commit}" >/dev/null 2>&1; then
              echo "Analyzing New Commit: ${new_hash}"
              git checkout -f "$new_hash"
              new_commit_date=$(git log -1 --format=%as)

              build_succeeded=false
              if mvn -B install -DskipTests -Drat.skip=true; then build_succeeded=true; fi
              if [ "$build_succeeded" = false ]; then
                echo "Initial build failed for new. Retrying with compile..."
                if mvn -B compile -Drat.skip=true; then build_succeeded=true; fi
              fi

              if [ "$build_succeeded" = true ]; then
                echo "Build succeeded for new. Running Sonar analysis..."
                if mvn -B sonar:sonar -Drat.skip=true -Dsonar.host.url=https://sonarcloud.io -Dsonar.projectKey=maven-issue-${issue_number} -Dsonar.projectName="Maven Issue #${issue_number}" -Dsonar.organization=${{ vars.SONAR_ORGANIZATION }} -Dsonar.projectVersion=2.0 -Dsonar.login=${{ secrets.SONAR_TOKEN }} -Dsonar.projectDate=${new_commit_date} -Dsonar.newCode.referenceBranch=1.0; then
                  echo "success" > ../../new_status.txt
                fi
              else
                echo "::error::All build attempts failed for new hash ${new_hash}."
              fi
            else
              echo "::warning::New commit ${new_hash} not found in repository mirror. Skipping."
            fi

            # --- START: API EXTRACTION AND REPORTING (Conditional) ---
            base_analysis_status=$(cat ../../base_status.txt)
            new_analysis_status=$(cat ../../new_status.txt)

            if [ "$base_analysis_status" = "success" ] && [ "$new_analysis_status" = "success" ]; then
              echo "Both analyses succeeded. Fetching API reports for Issue #${issue_number}"
              sleep 60

              # Get new issues with detailed fields
              curl -s -u "${{ secrets.SONAR_TOKEN }}:" "https://sonarcloud.io/api/issues/search?componentKeys=maven-issue-${issue_number}&sinceLeakPeriod=true&p=1&ps=500&f=key,rule,severity,type,component,line,message,effort,tags,creationDate,updateDate" -o "../../chunk-reports/new_issues_report_${issue_number}.json"
              
              # Get fixed issues with detailed fields
              curl -s -u "${{ secrets.SONAR_TOKEN }}:" "https://sonarcloud.io/api/issues/search?componentKeys=maven-issue-${issue_number}&statuses=RESOLVED,CLOSED&resolutions=FIXED&resolvedAfter=${base_commit_date}T00:00:00Z&p=1&ps=500&f=key,rule,severity,type,component,line,message,effort,tags,creationDate,updateDate,resolution" -o "../../chunk-reports/fixed_issues_report_${issue_number}.json"
            else
              echo "::warning::Skipping API extraction for Issue #${issue_number} due to analysis failure."
              echo "Base analysis status: ${base_analysis_status}"
              echo "New analysis status: ${new_analysis_status}"
            fi
            
            # Go back to the root of the runner's workspace
            cd ../..
            # Clean up the isolated directory and status files
            rm -rf "${work_dir}" base_status.txt new_status.txt
            
            echo "--- Finished processing Issue #${issue_number} ---"
          done

      - name: 'Upload Reports Artifact for this Chunk'
        uses: actions/upload-artifact@v4
        with:
          name: sonarqube-reports-chunk-${{ matrix.chunk_file }}
          path: chunk-reports/
