name: 'SonarCloud Historical Issue Analysis (Parallel)'

on:
  workflow_dispatch:

jobs:
  # Job 1: Splits the large issues.json file into smaller, manageable chunks.
  setup-chunks:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.create-matrix.outputs.matrix }}
    steps:
      - name: 'Checkout repository'
        uses: actions/checkout@v4

      - name: 'Install jq'
        run: sudo apt-get install -y jq

      - name: 'Debug: Check issues.json file'
        run: |
          if [ ! -f "issues.json" ]; then
            echo "::error::issues.json file not found!"
            exit 1
          fi
          echo "issues.json found. First 10 lines:"
          head -n 10 issues.json

      - name: 'Split JSON into chunks'
        id: create-matrix
        run: |
          mkdir -p chunks
          # Step 1: Process the JSON and save the output to a temporary file, one chunk per line.
          jq -c '. | to_entries | group_by(.key / 5 | floor) | .[] | map(.value)' issues.json > chunks.tmp

          # Step 2: Debugging - check if the temporary file has content.
          if [ ! -s chunks.tmp ]; then
            echo "::error::jq processing resulted in an empty file. Check issues.json structure."
            exit 1
          fi
          echo "Temporary chunk file created with $(wc -l < chunks.tmp) chunks."

          # Step 3: Read the temporary file and create the final chunk files.
          i=0
          while read -r chunk; do
            printf -v filename "chunks/part-%02d.json" "$i"
            echo "$chunk" > "$filename"
            echo "Created chunk file: $filename"
            i=$((i+1))
          done < chunks.tmp

          # Clean up the temporary file
          rm chunks.tmp
          
          # Check if any chunk files were created. If not, fail with an error.
          if [ ! "$(ls -A chunks)" ]; then
            echo "::error::No chunk files were created. issues.json might be empty or invalid."
            exit 1
          fi

          # Create a compact, single-line JSON array of the chunk filenames for the matrix strategy.
          matrix_json=$(ls chunks | jq -R . | jq -s -c .)
          echo "Generated matrix for next job: $matrix_json"
          echo "matrix=${matrix_json}" >> $GITHUB_OUTPUT

      - name: 'Upload chunks artifact'
        uses: actions/upload-artifact@v4
        with:
          name: issue-chunks
          path: chunks/

  # Job 2: Runs in parallel for each chunk file created above.
  analyze-chunk:
    runs-on: ubuntu-latest
    needs: setup-chunks
    strategy:
      fail-fast: false
      matrix:
        # The matrix is dynamically created from the list of chunk filenames.
        chunk_file: ${{ fromJson(needs.setup-chunks.outputs.matrix) }}

    steps:
      - name: 'Checkout repository'
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetch all history for git checkouts

      - name: 'Download issue chunks'
        uses: actions/download-artifact@v4
        with:
          name: issue-chunks
          path: chunks

      - name: 'Process each issue in a loop'
        run: |
          # The loop now reads from the specific chunk file assigned to this parallel job.
          jq -c '.[]' chunks/${{ matrix.chunk_file }} | while read issue; do
            # Extract details for the current issue using jq
            issue_number=$(echo "$issue" | jq -r '.number')
            base_hash=$(echo "$issue" | jq -r '.base_hash')
            new_hash=$(echo "$issue" | jq -r '.new_hash')

            echo "--- [${{ matrix.chunk_file }}] Processing Issue #${issue_number} ---"
            echo "Base Hash: ${base_hash}"
            echo "New Hash: ${new_hash}"

            # Use status files to track success/failure
            echo "failure" > base_status.txt
            echo "failure" > new_status.txt

            # --- START: BASE HASH ANALYSIS (Version 1.0) ---
            echo "Analyzing Base Commit: ${base_hash}"
            git checkout -f "$base_hash"
            base_commit_date=$(git log -1 --format=%as)
            
            # --- Build Step (Base) ---
            build_succeeded=false
            if mvn -B install -DskipTests -Drat.skip=true; then
              build_succeeded=true
            else
              echo "Initial build failed for base. Retrying with compile..."
              if mvn -B compile -Drat.skip=true; then
                build_succeeded=true
              fi
            fi

            # --- Analysis Step (Base) ---
            if [ "$build_succeeded" = true ]; then
              echo "Build succeeded for base. Running Sonar analysis..."
              if mvn -B sonar:sonar -Drat.skip=true -Dsonar.host.url=https://sonarcloud.io -Dsonar.projectKey=maven-issue-${issue_number} -Dsonar.projectName="Maven Issue #${issue_number}" -Dsonar.organization=${{ vars.SONAR_ORGANIZATION }} -Dsonar.projectVersion=1.0 -Dsonar.login=${{ secrets.SONAR_TOKEN }} -Dsonar.projectDate=${base_commit_date}; then
                echo "success" > base_status.txt
              fi
            else
              echo "::error::All build attempts failed for base hash ${base_hash}."
            fi

            # --- START: NEW HASH ANALYSIS (Version 2.0) ---
            echo "Analyzing New Commit: ${new_hash}"
            git checkout -f "$new_hash"
            new_commit_date=$(git log -1 --format=%as)

            # --- Build Step (New) ---
            build_succeeded=false
            if mvn -B install -DskipTests -Drat.skip=true; then
              build_succeeded=true
            else
              echo "Initial build failed for new. Retrying with compile..."
              if mvn -B compile -Drat.skip=true; then
                build_succeeded=true
              fi
            fi

            # --- Analysis Step (New) ---
            if [ "$build_succeeded" = true ]; then
              echo "Build succeeded for new. Running Sonar analysis..."
              if mvn -B sonar:sonar -Drat.skip=true -Dsonar.host.url=https://sonarcloud.io -Dsonar.projectKey=maven-issue-${issue_number} -Dsonar.projectName="Maven Issue #${issue_number}" -Dsonar.organization=${{ vars.SONAR_ORGANIZATION }} -Dsonar.projectVersion=2.0 -Dsonar.login=${{ secrets.SONAR_TOKEN }} -Dsonar.projectDate=${new_commit_date} -Dsonar.newCode.referenceBranch=1.0; then
                echo "success" > new_status.txt
              fi
            else
              echo "::error::All build attempts failed for new hash ${new_hash}."
            fi

            # --- START: API EXTRACTION AND REPORTING (Conditional) ---
            base_analysis_status=$(cat base_status.txt)
            new_analysis_status=$(cat new_status.txt)

            if [ "$base_analysis_status" = "success" ] && [ "$new_analysis_status" = "success" ]; then
              echo "Both analyses succeeded. Fetching API reports for Issue #${issue_number}"
              sleep 60

              curl -s -u "${{ secrets.SONAR_TOKEN }}:" "https://sonarcloud.io/api/issues/search?componentKeys=maven-issue-${issue_number}&sinceLeakPeriod=true&p=1&ps=500" -o "new_issues_report_${issue_number}.json"
              curl -s -u "${{ secrets.SONAR_TOKEN }}:" "https://sonarcloud.io/api/issues/search?componentKeys=maven-issue-${issue_number}&statuses=RESOLVED,CLOSED&resolutions=FIXED&resolvedAfter=${base_commit_date}T00:00:00Z&p=1&ps=500" -o "fixed_issues_report_${issue_number}.json"

              # Upload artifacts for THIS issue before the next loop iteration
              gh artifact upload "new_issues_report_${issue_number}.json" "fixed_issues_report_${issue_number}.json" --name "sonarqube-report-issue-${issue_number}"
            else
              echo "::warning::Skipping API extraction for Issue #${issue_number} due to analysis failure."
              echo "Base analysis status: ${base_analysis_status}"
              echo "New analysis status: ${new_analysis_status}"
            fi
            
            # Clean up status files for the next iteration
            rm -f base_status.txt new_status.txt

            echo "--- Finished processing Issue #${issue_number} ---"
            # Return to the original state to ensure clean checkout for the next iteration
            git checkout main # Or your default branch
          done
        env:
          # The GITHUB_TOKEN is required for the 'gh' CLI to upload artifacts
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
